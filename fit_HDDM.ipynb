{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fit_HDDM.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KW-plato/DriftDiffusionModel/blob/main/fit_HDDM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWRG9dqHwx3j"
      },
      "source": [
        "**Estimate DDM parameters using HDDM 0.6**\n",
        "\n",
        "code adapted from Anne Urai's fitHDDM.py script https://github.com/anne-urai/2019_Urai_choice-history-ddm/blob/master/fitHDDM.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cFRF1gjwutr"
      },
      "source": [
        "#!apt-get install python3.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr2znaiRnwEJ"
      },
      "source": [
        "#!pip install pymc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSMGCgzEo_xO"
      },
      "source": [
        "#!pip install pandas patsy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA7fZWkSpIIN"
      },
      "source": [
        "#!pip install kabuki"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d349zVjfpi-r"
      },
      "source": [
        "#!pip install hddm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZSNRMVxpxWg"
      },
      "source": [
        "import glob\n",
        "import hddm\n",
        "import kabuki\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQfqPjc5y9r0",
        "outputId": "15c967eb-e121-41ab-d3b2-b2e141db6fd6"
      },
      "source": [
        "print(hddm.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I54wfl6oq7sD"
      },
      "source": [
        "def recode_4stimcoding(mydata):\n",
        "    #code stimulus and response direction left as 0, leave direction right as 1.\n",
        "    \n",
        "    mydata.loc[mydata['direction']==-1,'direction'] = 0\n",
        "    mydata.loc[mydata['response']==-1,'response'] = 0\n",
        "    for col in mydata.columns.tolist():\n",
        "        if ('stim' in col) or ('resp' in col):\n",
        "            mydata.loc[mydata[col]==-1,col] = 0\n",
        "    \n",
        "    return mydata\n",
        "\n",
        "def z_link_func(x):\n",
        "    return 1 / (1 + np.exp(-(x.values.ravel())))\n",
        "\n",
        "def aic(self):\n",
        "    k = len(self.get_stochastics())\n",
        "    logp = sum([x.logp for x in self.get_observeds()['node']])\n",
        "    return 2 * k - 2 * logp\n",
        "\n",
        "\n",
        "def bic(self):\n",
        "    k = len(self.get_stochastics())\n",
        "    n = len(self.data)\n",
        "    logp = sum([x.logp for x in self.get_observeds()['node']])\n",
        "    return -2 * logp + k * np.log(n)\n",
        "\n",
        "def concat_models(mypath, model_name, nchains=30):\n",
        "    traces = range(1,nchains+1)\n",
        "\n",
        "    # CHECK IF COMBINED MODEL EXISTS\n",
        "    if os.path.isfile(os.path.join(mypath, model_name, 'modelfit-combined.model')):\n",
        "        print(\"Combined Model exists: {}\".format(os.path.join(mypath, model_name, 'modelfit-combined.model')))\n",
        "    else:\n",
        "        # ============================================ #\n",
        "        # APPEND MODELS\n",
        "        # ============================================ #\n",
        "        allmodels = []\n",
        "        print(\"Combining all traces for %s\" % model_name)\n",
        "        for trace_id in traces:  # how many chains were run?\n",
        "            model_filename = os.path.join(mypath, model_name, 'modelfit-md%d.model' % trace_id)\n",
        "            if os.path.isfile(model_filename) == True:  # if not, this model has to be rerun\n",
        "                print(model_filename)\n",
        "                thism = hddm.load(model_filename)\n",
        "                allmodels.append(thism)  # now append into a list\n",
        "            else:\n",
        "                print(\"Not found: trace_id {:2d}\".format(trace_id))\n",
        "                \n",
        "        if len(allmodels) != nchains:\n",
        "            return None\n",
        "        # ============================================ #\n",
        "        # CHECK CONVERGENCE if all traces were found\n",
        "        # ============================================ #    \n",
        "        print(\"Performing gelman rubin convergence test\\n\")\n",
        "        try:\n",
        "            gr = hddm.analyze.gelman_rubin(allmodels)\n",
        "            # save\n",
        "            text_file = open(os.path.join(mypath, model_name, 'gelman_rubin.txt'), 'w')\n",
        "            for p in gr.items():\n",
        "                text_file.write(\"%s,%s\\n\" % p)\n",
        "                # print a warning when non-convergence is detected\n",
        "                # Values should be close to 1 and not larger than 1.02 which would indicate convergence problems.\n",
        "                # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3731670/\n",
        "                if abs(p[1] - 1) > 0.02:\n",
        "                    print(\"non-convergence found, %s:%s\" % p)\n",
        "            text_file.close()\n",
        "            print(\"written gelman rubin stats to file\")\n",
        "        except Exception as e:\n",
        "            print(\"Error: {}\".format(e))\n",
        "        m = kabuki.utils.concat_models(allmodels) #creates one model from all chains\n",
        "\n",
        "        # ============================================ #\n",
        "        # SAVE THE FULL MODEL\n",
        "        # ============================================ #\n",
        "\n",
        "        m.save(os.path.join(mypath, model_name, 'modelfit-combined.model'))  # save combined modelto disk\n",
        "        print(\"Concatenated model saved!\")\n",
        "        \n",
        "        # ============================================ #\n",
        "        # SAVE POINT ESTIMATES\n",
        "        # ============================================ #\n",
        "\n",
        "        print(\"saving stats...\")\n",
        "        results = m.gen_stats()  # point estimate for each parameter and subject\n",
        "        results.to_csv(os.path.join(mypath, model_name, 'results-combined.csv'))\n",
        "\n",
        "        # save the DIC for this model\n",
        "        text_file = open(os.path.join(mypath, model_name, 'DIC-combined.txt'), 'w')\n",
        "        text_file.write(\"Combined model: {}\\n\".format(m.dic))\n",
        "        text_file.close()\n",
        "        print('done')\n",
        "        \n",
        "        # ============================================ #\n",
        "        # SAVE TRACES\n",
        "        # ============================================ #\n",
        "\n",
        "        print(\"saving traces...\")\n",
        "        # get the names for all nodes that are available here\n",
        "        group_traces = m.get_group_traces()\n",
        "        group_traces.to_csv(os.path.join(mypath, model_name, 'group_traces.csv'))\n",
        "\n",
        "        all_traces = m.get_traces()\n",
        "        all_traces.to_csv(os.path.join(mypath, model_name, 'all_traces.csv'))\n",
        "        print('done')\n",
        "        \n",
        "        # ============================================ #\n",
        "        # CONCATENATE MODEL COMPARISON\n",
        "        # ============================================ #\n",
        "        # average model comparison values across chains\n",
        "        print('concatenating model comparison...')\n",
        "        fls = glob.glob(os.path.join(mypath, model_name, 'model_comparison_md*.csv'))\n",
        "        tmpdf = pd.concat([pd.read_csv(f) for f in fls ])\n",
        "        # average over chains\n",
        "        df2 = tmpdf.mean()\n",
        "        df2 = tmpdf.describe().loc[['mean']]\n",
        "        df2.to_csv(os.path.join(mypath, model_name, 'model_comparison.csv')) # save comparison to disk\n",
        "        print('done')\n",
        "\n",
        "\n",
        "        # DELETE FILES to save space\n",
        "        print(\"Now deleting files for seperate chains...\")\n",
        "        for fl in glob.glob(os.path.join(mypath, model_name, 'modelfit-md*.model')):\n",
        "            print(fl)\n",
        "            os.remove(fl)\n",
        "        for fl in glob.glob(os.path.join(mypath, model_name, 'modelfit-md*.db')):\n",
        "            if not '-md1.db' in fl: #needed here to load the pickled db in PPC\n",
        "                print(fl)\n",
        "                os.remove(fl)\n",
        "        for fl in glob.glob(os.path.join(mypath, model_name, 'model_comparison_md*.csv')):\n",
        "            print(fl)\n",
        "            os.remove(fl)\n",
        "        for fl in glob.glob(os.path.join(mypath, model_name, 'DIC-md*.txt')):\n",
        "            print(fl)\n",
        "            os.remove(fl)\n",
        "        print('DONE!!!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg3KavV4rS5F"
      },
      "source": [
        "def make_model(mydata, model_name, trace_id, nlag=0):\n",
        "    \n",
        "    #checks before model is created\n",
        "    if \"nohist\" not in model_name and nlag == 0:\n",
        "        print(\"For all models with history effects, 'nlag' must be non-zero\")\n",
        "        exit(0)\n",
        "    elif \"nohist\" in model_name and nlag != 0:\n",
        "        print(\"'nlag' specified but model is without history effect. 'nlag' value is ignored\\n\")\n",
        "        nlag = 0\n",
        "         \n",
        "    if 'regress' in model_name:\n",
        "        if nlag != 0:\n",
        "            lags = range(1,nlag+1)\n",
        "            resp_cols = ['l' + str(i) + '_resp' for i in lags]\n",
        "            stim_cols = ['l' + str(i) + '_stim' for i in lags]\n",
        "            resps = \" + \".join(resp_cols)\n",
        "            respstim = \" + \".join(resp_cols + stim_cols)\n",
        "            for col in stim_cols:\n",
        "                mydata = mydata[mydata[col].notna()]\n",
        "    else:\n",
        "        mydata = recode_4stimcoding(mydata)\n",
        "        if nlag != 0:\n",
        "            col_resp = 'l' + str(nlag) + '_resp'\n",
        "            col_stim = 'l' + str(nlag) + '_stim'\n",
        "            col_stim = 'l' + str(nlag) + '_subject'\n",
        "            col_repeat = 'l' + str(nlag) + '_repeat'\n",
        "             \n",
        "    if model_name == 'stimcoding_nohist': # NO HISTORY FOR MODEL COMPARISON\n",
        "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
        "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
        "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'])\n",
        "    elif model_name == 'stimcoding_dc_resp':\n",
        "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
        "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
        "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
        "                depends_on={'dc':[col_resp]})\n",
        "    elif model_name == 'stimcoding_z_resp':\n",
        "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
        "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
        "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
        "                depends_on={'z':[col_resp]})\n",
        "    elif model_name == 'stimcoding_dc_z_resp':\n",
        "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
        "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
        "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
        "                depends_on={'dc':[col_resp], 'z':[col_resp]})\n",
        "    elif model_name == 'stimcoding_dc_z_st_resp': # also estimate across-trial variability in nondecision time\n",
        "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
        "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
        "                include=('sv', 'sz', 'st'), group_only_nodes=['sv', 'sz', 'st'],\n",
        "                depends_on={'dc':[col_resp], 'z':[col_resp]})\n",
        "    # ============================================ #\n",
        "    # Dyadic models..depends on whether the lagged trial was own for partner's\n",
        "    # ============================================ #\n",
        "    elif model_name == 'stimcoding_dc_z_resp_dyadic':\n",
        "        m = hddm.HDDMStimCoding(mydata, stim_col='direction', split_param='v',\n",
        "                drift_criterion=True, bias=True, p_outlier=0.05,\n",
        "                include=('sv', 'sz'), group_only_nodes=['sv', 'sz'],\n",
        "                depends_on={'dc':[col_resp, col_subject], 'z':[col_resp, col_subject]})\n",
        "    \n",
        "    return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HrjepX_sPQC"
      },
      "source": [
        "def get_full_model_name(m,lag):\n",
        "    return m if 'nohist' in m else m + '_l' + str(lag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIEl9CtwsTQe"
      },
      "source": [
        "def run_model(m, mypath, model_name, trace_id, n_samples):\n",
        "    print(\"Running {:<s}, trace_id {:2d}\".format(model_name,trace_id))\n",
        "    print(\"finding starting values\")\n",
        "    try:\n",
        "        m.find_starting_values() # this should help the sampling\n",
        "    except Exception as e:\n",
        "        print(e) #even if starting values couldnt be found, sampling can continue\n",
        "\n",
        "    print(\"begin sampling\")\n",
        "    #m.sample(n_samples, burn=n_samples/2, thin=3)\n",
        "    \n",
        "    m.sample(n_samples, burn=n_samples/2, thin=3, db='pickle',\n",
        "        dbname=os.path.join(mypath, model_name, 'modelfit-md%d.db'%trace_id))\n",
        "    m.save(os.path.join(mypath, model_name, 'modelfit-md%d.model'%trace_id)) # save the model to disk\n",
        "    \n",
        "    # ============================================ #\n",
        "    # save the output values\n",
        "    # ============================================ #\n",
        "\n",
        "    # save the DIC for this model\n",
        "    text_file = open(os.path.join(mypath, model_name, 'DIC-md%d.txt'%trace_id), 'w')\n",
        "    text_file.write(\"Model {}: {}\\n\".format(trace_id, m.dic))\n",
        "    text_file.close()\n",
        "\n",
        "    # save the other model comparison indices\n",
        "    df = dict()\n",
        "    df['trace_id'] = trace_id\n",
        "    df['dic_original'] = [m.dic]\n",
        "    df['aic'] = [aic(m)]\n",
        "    df['bic'] = [bic(m)]\n",
        "    df2 = pd.DataFrame(df)\n",
        "    df2.to_csv(os.path.join(mypath, model_name, 'model_comparison_md%d.csv'%trace_id),index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iam7wGUZti-W"
      },
      "source": [
        "def recode_subjidx_lag_trials(df,nprev):\n",
        "    \n",
        "    colname = 'l' + str(nprev) + '_subject'\n",
        "    df[colname] = (df[colname] == df['subj_idx']) * 1\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwh8ys6Rsb7Q"
      },
      "source": [
        "models_collection = {\n",
        "    1: 'stimcoding_nohist',  # no history baseline model\n",
        "    2: 'stimcoding_dc_resp',  #previous response dependent, nlags needed\n",
        "    3: 'stimcoding_z_resp',  #previous response dependent, nlags needed\n",
        "    4: 'stimcoding_dc_z_resp', #previous response dependent, nlags needed\n",
        "    5: 'stimcoding_dc_z_st_resp',  #previous response dependent, nlags needed\n",
        "    6: 'stimcoding_dc_z_resp_dyadic' #previous response dependent, nlags needed, dyadic assessment\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI4OohhismGa"
      },
      "source": [
        "\"\"\"\n",
        "Prepare for running: Part I\n",
        "Select model and the nth prev trial intended to be analyzed\n",
        "\"\"\"\n",
        "model = models_collection[19]\n",
        "lag = 1\n",
        "chains = 2\n",
        "trace_ids = range(1,chains+1)\n",
        "n_samples = 50\n",
        "\n",
        "mypath = \"output\"\n",
        "full_model_name = get_full_model_name(model,lag)\n",
        "thispath = os.path.join(mypath,full_model_name)\n",
        "if not os.path.exists(thispath):\n",
        "    os.makedirs(thispath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8pboiket-xK"
      },
      "source": [
        "\"\"\"\n",
        "Prepare for running:Part II\n",
        "Setup the folders and fetch data\n",
        "\"\"\"\n",
        "mypath = \"output\"\n",
        "datasrc = \"data/coded\"\n",
        "datafile = \"all_trials_data.csv\"\n",
        "\n",
        "#if the concatenated data from each experimnetal run exists, use that, else create the concatenated data first\n",
        "data_loc = os.path.join(datasrc,datafile)\n",
        "if os.path.exists(data_loc):\n",
        "    mydata = hddm.load_csv(data_loc)\n",
        "    print(\"Concatenated datafile with full experiment data found and loaded\")\n",
        "else:\n",
        "    files = glob.glob(os.path.join(datasrc,'pair*.csv'))\n",
        "    mydata = pd.concat([pd.read_csv(f) for f in files ])\n",
        "    mydata.to_csv(data_loc,index=False,header=True)\n",
        "    print(\"Concatenated datafile with full experiment data created\")\n",
        "    \n",
        "\n",
        "#DDM param estimations are done for 3 conditions: \n",
        "# 1. No history condition\n",
        "# 2. depends on the lagged trial response \n",
        "# 3. depends on the lagged trial response and own (code 1) or partner (code 0)trial\n",
        "\n",
        "mydata = recode_subjidx_lag_trials(mydata,lag)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FIBl8iQsvJz"
      },
      "source": [
        "# ============================================ #\n",
        "# Main HDDM parameter estimation\n",
        "# ============================================ #\n",
        "starttime = time.time()\n",
        "for trace_id in trace_ids:\n",
        "    model_filename = os.path.join(mypath, full_model_name, 'modelfit-md%d.model' % trace_id)\n",
        "    m = make_model(mydata, model, trace_id, lag)\n",
        "    run_model(m, mypath, full_model_name, trace_id, n_samples)\n",
        "    elapsed = time.time() - starttime\n",
        "    print(\"\\nElapsed time for %s, trace_id %d, %d samples: %f seconds\\n\" % (model, trace_id, n_samples, elapsed))\n",
        "\n",
        "concat_models(mypath, full_model_name, chains)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqNAJn--sw0d"
      },
      "source": [
        "# ============================================ #\n",
        "# POSTERIOR PREDICTIVES TO ASSESS MODEL FIT\n",
        "# ============================================ #\n",
        "\n",
        "starttime = time.time()\n",
        "print(\"computing ppc\")\n",
        "# specify how many samples are needed\n",
        "m = hddm.load(os.path.join(mypath,full_model_name, 'modelfit-combined.model'))\n",
        "nsmp = 100\n",
        "ppc = hddm.utils.post_pred_gen(m, append_data=True, samples=nsmp)\n",
        "\n",
        "# make the csv smaller, save disk space\n",
        "savecols = list(set(ppc.columns) & set(['rt','rt_sampled', 'response_sampled',\n",
        "                        'index', 'direction', 'response', 'l1_resp', 'subj_idx']))\n",
        "ppc = ppc[savecols]\n",
        "# save as pandas dataframe\n",
        "ppc.to_csv(os.path.join(mypath, full_model_name, 'ppc_data.csv'), index=True)\n",
        "elapsed = time.time() - starttime\n",
        "print( \"\\nElapsed time for %s, PPC: %f seconds\\n\" %(full_model_name,elapsed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H96p5mrOs0ei"
      },
      "source": [
        "# ============================================ #\n",
        "# QUANTILE OPTIMISATION\n",
        "# http://ski.clps.brown.edu/hddm_docs/howto.html#run-quantile-opimization\n",
        "# ============================================ #\n",
        "\n",
        "subj_params = []\n",
        "bic = []\n",
        "\n",
        "for subj_idx, subj_data in mydata.groupby('subj_idx'):\n",
        "    m_subj = make_model(subj_data, model, 1,lag)\n",
        "    thismodel = m_subj.optimize('gsquare', quantiles=[0.1, 0.3, 0.5, 0.7, 0.9], n_runs=5)\n",
        "    thismodel.update({'subj_idx':subj_idx}) # keep original subject number\n",
        "    subj_params.append(thismodel)\n",
        "    bic.append(m_subj.bic_info)\n",
        "\n",
        "params = pd.DataFrame(subj_params)\n",
        "params.to_csv(os.path.join(mypath, full_model_name, 'Gsquare.csv'))\n",
        "bic = pd.DataFrame(bic)\n",
        "bic.to_csv(os.path.join(mypath, full_model_name, 'BIC.csv'))\n",
        "print(\"QUANTILE OPTIMISATION. DONE!!!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}